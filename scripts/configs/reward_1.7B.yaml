seed: 42
model_name_or_path: /w/exp/mkt/model_1.7B_BI_MT_02-2nd/checkpoint-100
data_dir: /w/mkt/data/kobaco/comparisons
max_length: 1024
num_proc: 1
eos_token: "[EOS]"

trainer:
  output_dir: /w/exp/mkt/reward_1.7B_BI_MT_02
  num_train_epochs: 3
  
  learning_rate: 0.0000008
  lr_scheduler_type: cosine
  warmup_ratio: 0.1
  
  per_device_train_batch_size: 128
  per_device_eval_batch_size: 128
  
  evaluation_strategy: steps
  eval_steps: 5
  
  save_strategy: steps
  save_steps: 50
  
  load_best_model_at_end: true
  
  logging_strategy: steps
  logging_first_step: true
  logging_steps: 1
  
  report_to: wandb  
  deepspeed: configs/stage3.json

wandb:
  group: reward-1.7B-lr_${trainer.learning_rate}
  project: mkt
  entity: frl-lgai